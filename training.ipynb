{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "dbunk.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "80U8r-AXn0cG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!echo '{\"username\":\"mcparadip\",\"key\":\"ee60bb4deb5f1780508fbc0a864ea7dd\"}' > ~/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "!mkdir -p /content/input\n",
        "!cd /content/input\n",
        "!kaggle datasets download --path /content/input --unzip rtatman/glove-global-vectors-for-word-representation\n",
        "!cd /content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "mfpRqEVp7boM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import nltk\n",
        "import random\n",
        "import string\n",
        "import re\n",
        "import os\n",
        "\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras import Sequential\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ur7Siyq3geX1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "BATCH_SIZE = 128"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-6SMiThKhXv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LABELS = [\n",
        "    \"rumor\",\n",
        "    \"hate\",\n",
        "    \"unreliable\",\n",
        "    \"conspiracy\",\n",
        "    \"clickbait\",\n",
        "    \"satire\",\n",
        "    \"fake\",\n",
        "    \"reliable\",\n",
        "    \"bias\",\n",
        "    \"political\",\n",
        "    \"junksci\",\n",
        "    \"unknown\",\n",
        "]\n",
        "\n",
        "SPLIT = [\n",
        "    [\"fake\"],\n",
        "    # [\"political\"],\n",
        "    [\"reliable\"],\n",
        "    [\"rumor\", \"unknown\", \"hate\", \"clickbait\", \"conspiracy\", \"junksci\", \"satire\", \"bias\", \"unreliable\", \"political\"],\n",
        "]\n",
        "\n",
        "SPLIT = {\n",
        "    LABELS.index(x): idx\n",
        "    for idx, category in enumerate(SPLIT)\n",
        "    for x in category\n",
        "}\n",
        "\n",
        "SPLIT = tf.lookup.StaticHashTable(\n",
        "    tf.lookup.KeyValueTensorInitializer(\n",
        "        list(SPLIT.keys()),\n",
        "        list(SPLIT.values()),\n",
        "        key_dtype=tf.int64\n",
        "    ),\n",
        "    0\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "131lwkS6Zc84",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GCS_PATH = \"gs://dbunk\"\n",
        "files = tf.io.gfile.glob(GCS_PATH + \"/*.tfrecord\")\n",
        "files_train = files[:20]\n",
        "files_test = files[129:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikrOe4mfZhpx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature_description = {\n",
        "    \"text\": tf.io.FixedLenFeature([], tf.string),\n",
        "    \"label\": tf.io.FixedLenFeature([], tf.int64),\n",
        "}\n",
        "\n",
        "def parse(example):\n",
        "    parsed = tf.io.parse_single_example(example, feature_description)\n",
        "    return parsed[\"text\"], parsed[\"label\"]\n",
        "\n",
        "def simplify(x, y):\n",
        "    return x, SPLIT.lookup(y)\n",
        "\n",
        "def remove(x, y):\n",
        "    return y != 2\n",
        "\n",
        "ds_train = tf.data.TFRecordDataset(files_train, num_parallel_reads=AUTO)\n",
        "ds_train = ds_train.map(parse).map(simplify).filter(remove)\n",
        "ds_train = ds_train.shuffle(10000).batch(BATCH_SIZE).prefetch(AUTO)\n",
        "\n",
        "ds_test = tf.data.TFRecordDataset(files_test, num_parallel_reads=AUTO)\n",
        "ds_test = ds_test.map(parse).map(simplify).filter(remove)\n",
        "ds_test = ds_test.shuffle(10000).batch(BATCH_SIZE).prefetch(AUTO)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "FpDhKL3u7boR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_TOKENS = 5000\n",
        "OUTPUT_LEN = 300\n",
        "EMBEDDING_DIM = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "-7G--Gms7boY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_text(input_data):\n",
        "    data = tf.strings.lower(input_data)\n",
        "    data = tf.strings.regex_replace(data, r\"\\[[^]]*\\]\", \"\")\n",
        "    data = tf.strings.regex_replace(data, r\"http\\S+\", \"\")\n",
        "    data = tf.strings.regex_replace(data, f\"[{re.escape(string.punctuation)}]\", \"\")\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkpZcIJc-Uza",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectorize_layer = TextVectorization(\n",
        "    standardize=process_text,\n",
        "    max_tokens=MAX_TOKENS,\n",
        "    output_sequence_length=OUTPUT_LEN\n",
        ")\n",
        "train_text = ds_train.map(lambda x, y: x)\n",
        "vectorize_layer.adapt(train_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gvcq0b7DpL2K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "voc = vectorize_layer.get_vocabulary()\n",
        "word_index = dict(zip(voc, range(2, len(voc))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gueOCe5CpOwY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeddings_index = {}\n",
        "with open(\"input/glove.6B.100d.txt\") as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lPp5DVwpYqW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_tokens = len(voc) + 2\n",
        "embedding_dim = 100\n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "# Prepare embedding matrix\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word.decode(\"utf-8\"))\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in embedding index will be all-zeros.\n",
        "        # This includes the representation for \"padding\" and \"OOV\"\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        hits += 1\n",
        "    else:\n",
        "        misses += 1\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSnVzm4BWaII",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_weight = {0: 2, 1: 1}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIWbOtg9lVRe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n",
        "model.add(vectorize_layer)\n",
        "model.add(Embedding(\n",
        "    num_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=False,\n",
        "))\n",
        "model.add(LSTM(512, return_sequences=True))\n",
        "model.add(LSTM(512))\n",
        "model.add(Dense(32, activation=\"relu\"))\n",
        "model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(lr=0.001)\n",
        "loss = tf.keras.losses.BinaryCrossentropy()\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McU0vxPNapyU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-ZGw-ujaqWs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau()\n",
        "history = model.fit(\n",
        "    ds_train,\n",
        "    epochs=20,\n",
        "    steps_per_epoch=10240,\n",
        "    validation_data=ds_test,\n",
        "    class_weight=class_weight,\n",
        "    callbacks=[reduce_lr]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUJvZirCeXEz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y31y7AUsY3N7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSuOoU8XFsGN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.evaluate(ds_test.take(100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sh2l9pj0-dG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = \"\"\"Trump Trump News\"\"\"\n",
        "model.predict([a])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rn4ylcR6ZQRb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}